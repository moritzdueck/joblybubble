{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3e6cd6",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abadef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd5f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from json2html import json2html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a5bcc",
   "metadata": {},
   "source": [
    "# Job Listing URL Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ed433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import random\n",
    "\n",
    "user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "]\n",
    "\n",
    "#headers={\"User-Agent\": \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"}\n",
    "\n",
    "\n",
    "def get_indeed_search_url(keyword, location, offset=0):\n",
    "    parameters = {\"q\": keyword, \"l\": location, \"filter\": 0, \"start\": offset}\n",
    "    return \"https://www.indeed.com/jobs?\" + urlencode(parameters)\n",
    "\n",
    "def get_indeed_job_urls(keyword_list, location_list):\n",
    "\n",
    "    job_id_list = []\n",
    "    job_url_list = []\n",
    "\n",
    "    ## Loop Through Indeed Pages Until No More Jobs\n",
    "    for keyword in keyword_list:\n",
    "        for location in location_list:\n",
    "            for offset in range(0, 1010, 10):\n",
    "                try:\n",
    "                    indeed_jobs_url = get_indeed_search_url(keyword, location, offset)\n",
    "                    \n",
    "                    headers={'User-Agent': random.choice(user_agents_list)}\n",
    "                    \n",
    "                    response = requests.get(indeed_jobs_url, headers=headers)\n",
    "                    print(response)\n",
    "\n",
    "                    if response.status_code == 200:\n",
    "                        script_tag  = re.findall(r'window.mosaic.providerData\\[\"mosaic-provider-jobcards\"\\]=(\\{.+?\\});', response.text)\n",
    "                        if script_tag is not None:\n",
    "                            json_blob = json.loads(script_tag[0])\n",
    "                            jobs_list = json_blob['metaData']['mosaicProviderJobCardsModel']['results']\n",
    "                            for index, job in enumerate(jobs_list):\n",
    "                                if job.get('jobkey') is not None:\n",
    "                                    job_id_list.append(job.get('jobkey'))\n",
    "\n",
    "                            ## If response contains less than 10 jobs then stop pagination\n",
    "                            if len(jobs_list) < 10:\n",
    "                                break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('Error', e)\n",
    "    \n",
    "    print(job_id_list)\n",
    "                    \n",
    "     \n",
    "    for idx in job_id_list:\n",
    "        indeed_job_url = 'https://www.indeed.com/m/basecamp/viewjob?viewtype=embedded&jk=' + idx\n",
    "        job_url_list.append(indeed_job_url)\n",
    "\n",
    "    return job_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05f2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m keyword_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftware engineer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m location_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_indeed_job_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m, in \u001b[0;36mget_indeed_job_urls\u001b[1;34m(keyword_list, location_list)\u001b[0m\n\u001b[0;32m     31\u001b[0m indeed_jobs_url \u001b[38;5;241m=\u001b[39m get_indeed_search_url(keyword, location, offset)\n\u001b[0;32m     33\u001b[0m headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: random\u001b[38;5;241m.\u001b[39mchoice(user_agents_list)}\n\u001b[1;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindeed_jobs_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\sessions.py:577\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    573\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 577\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_environment_settings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\sessions.py:759\u001b[0m, in \u001b[0;36mSession.merge_environment_settings\u001b[1;34m(self, url, proxies, stream, verify, cert)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrust_env:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# Set environment's proxies.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     no_proxy \u001b[38;5;241m=\u001b[39m proxies\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m proxies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     env_proxies \u001b[38;5;241m=\u001b[39m \u001b[43mget_environ_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_proxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m env_proxies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    761\u001b[0m         proxies\u001b[38;5;241m.\u001b[39msetdefault(k, v)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\utils.py:830\u001b[0m, in \u001b[0;36mget_environ_proxies\u001b[1;34m(url, no_proxy)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_environ_proxies\u001b[39m(url, no_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    825\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;124;03m    Return a dict of environment proxies.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \n\u001b[0;32m    828\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mshould_bypass_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_proxy\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\utils.py:814\u001b[0m, in \u001b[0;36mshould_bypass_proxies\u001b[1;34m(url, no_proxy)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_environ(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m, no_proxy_arg):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;66;03m# parsed.hostname can be `None` in cases such as a file URI.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m         bypass \u001b[38;5;241m=\u001b[39m \u001b[43mproxy_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, socket\u001b[38;5;241m.\u001b[39mgaierror):\n\u001b[0;32m    816\u001b[0m         bypass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\utils.py:121\u001b[0m, in \u001b[0;36mproxy_bypass\u001b[1;34m(host)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proxy_bypass_environment(host)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_bypass_registry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\requests\\utils.py:83\u001b[0m, in \u001b[0;36mproxy_bypass_registry\u001b[1;34m(host)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     internetSettings \u001b[38;5;241m=\u001b[39m \u001b[43mwinreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenKey\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwinreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHKEY_CURRENT_USER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSoftware\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMicrosoft\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWindows\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCurrentVersion\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mInternet Settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# ProxyEnable could be REG_SZ or REG_DWORD, normalizing it\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     proxyEnable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(winreg\u001b[38;5;241m.\u001b[39mQueryValueEx(internetSettings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProxyEnable\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    ## Job Search Parameters\n",
    "    keyword_list = ['software engineer']\n",
    "    location_list = ['California']\n",
    "    ids = get_indeed_job_urls(keyword_list, location_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c47aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f49fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc738c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a501a6f",
   "metadata": {},
   "source": [
    "# Static parameters\n",
    "These parameters are used to filter criteria for the [ClimateBase.org Jobs](https://climatebase.org/jobs?l=&q=&p=0&remote=false) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://climatebase.org/jobs?l=&q=&p=0&remote=false\n",
    "domain_name = \"https://climatebase.org\"\n",
    "url_path = \"/jobs?l=&q=&p=0&remote=false\"\n",
    "\n",
    "# Job types\n",
    "#https://climatebase.org/jobs?l=&q=&job_types=Full+time+role&p=0&remote=false\n",
    "d_job_types  = {0:\"\", 1:\"Full+time+role\", 2:\"Internship\"}\n",
    "\n",
    "# Role type\n",
    "#https://climatebase.org/jobs?l=&q=&categories=Data+Analyst&p=0&remote=false\n",
    "d_categories = {0:\"\", 1:\"Data+Analyst\", 2:\"Data+Scientist\", 3:\"Research\"}\n",
    "\n",
    "# Remote\n",
    "#https://climatebase.org/jobs?l=Remote&q=&p=0&remote=true\n",
    "d_remote = {0:\"\", 1:\"true\", 2:\"false\"}\n",
    "\n",
    "css_object_class = \"list_card\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7afdc",
   "metadata": {},
   "source": [
    "# User-defined parameters\n",
    "These parameters are the filtering criteria for the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dd2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job type: Fulltimerole\n",
      "Category: DataScientist\n",
      "Remote: true\n"
     ]
    }
   ],
   "source": [
    "job_types = d_job_types[1]\n",
    "print(\"Job type: \" + job_types.replace(\"+\", \"\"))\n",
    "\n",
    "categories = d_categories[2]\n",
    "print(\"Category: \" + categories.replace(\"+\", \"\"))\n",
    "\n",
    "remote = d_remote[1]\n",
    "print(\"Remote: \" + remote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb383015",
   "metadata": {},
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ded7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_filter(input_text, to_insert):\n",
    "    \"\"\"\n",
    "    This function formats the url structure to make a filtered query.\n",
    "    \"\"\"\n",
    "    # Find the index where \"&p=\" starts\n",
    "    index = input_text.find(\"&p=\")\n",
    "\n",
    "    # Insert the text to the left of \"&p=\"\n",
    "    new_string = input_text[:index] + to_insert + input_text[index:]\n",
    "    \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59b0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_remote(input_text):\n",
    "    \"\"\"\n",
    "    This function is similar to insert_filter(), but is specific for the \"remote\" filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_string = input_text.replace(\"?l=\", \"?l=Remote\")\n",
    "    new_string = input_text.replace(\"&remote=false\", \"&remote=true\")\n",
    "\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d723b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_css_object(url_path, css_object_class):\n",
    "    \"\"\"\n",
    "    Given a CSS object class, this scraper will obtain the relevant information from the website.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = domain_name + url_path\n",
    "    #\"https://climatebase.org/jobs?l=&q=&categories=Data+Scientist&p=0&remote=true\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all elements with class=\"list_card\"\n",
    "    found_objects = soup.find_all(class_=css_object_class)\n",
    "\n",
    "    return found_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decb6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title(current_path):\n",
    "    \"\"\"\n",
    "    This function will obtain the job title from a predefined CSS object specific to \n",
    "    the ClimateBase.org website.\n",
    "    \"\"\"\n",
    "    \n",
    "    html_title = scraping_css_object(current_path, \"fcPVcr\")\n",
    "    soup = BeautifulSoup(str(html_title), 'html.parser')\n",
    "    title = soup.find('h1', {'class': 'PageLayout__Title-sc-1ri9r3s-4 fcPVcr'}).text\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b05128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_description(current_path):\n",
    "    \"\"\"\n",
    "    This function will obtain the job description from a predefined CSS object specific to\n",
    "    the ClimateBase.org website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mining job description\n",
    "    html_bodytext = scraping_css_object(current_path, \"EPUZp\")\n",
    "    soup = BeautifulSoup(str(html_bodytext), 'html.parser')\n",
    "    bodytext = soup.div.text.strip()\n",
    "\n",
    "    return bodytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025141a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_prompt(title, bodytext):\n",
    "    \"\"\"\n",
    "    This function contains the prompt with the set of rules that are to be sent to ChatGPT to process a text.\n",
    "    \"\"\"    \n",
    "    \n",
    "    categories = \"\"\"\n",
    "    * Job title\n",
    "    * Company name\n",
    "    * Company mission\n",
    "    * Company values \n",
    "    * Company products or services\n",
    "    * Job responsibilities\n",
    "    * Desired software skills\n",
    "    * Education\n",
    "    * Required Job Experience\n",
    "    * Equal Employment Opportunity\n",
    "    * Salary\n",
    "    * Benefits\n",
    "    * Location\n",
    "    * Type of employment\n",
    "    * URL\n",
    "    \"\"\"\n",
    "    json_keys = [category.strip('* ').lower().replace(' ', '_') for category in categories.strip().splitlines()]\n",
    "\n",
    "    text_prompt = f\"\"\"I will prompt you with a job description contained within ```, and I want your help to extract and categorize its information. Before we begin, please follow these rules: \n",
    "\n",
    "    1. Replace any double quotes in the text with single quotes.\n",
    "    2. Extract and categorize the information from the job description for the following categories:{categories}\n",
    "    3. Please provide your answers in a JSON object format. The keys will be the same as the categories but in lower case and with spaces replaced by underscores. These are respectively and in order: {json_keys}.\n",
    "    4. Use a consistent structure for all data entries. Never create nested values. Separate them with a delimiter such as \";\" instead.\n",
    "    5. If any category has no available information, please include a \"null\" value for the corresponding key in the JSON object. \n",
    "    6. Make the categorizations as concise as possible, maybe even as keywords. Be as economic as possible.\n",
    "    7. Avoid paragraphs of text or long sentences. \n",
    "    8. Avoid redundant text.\n",
    "\n",
    "    Please keep these rules in mind when categorizing the job description. Let's begin!: \n",
    "    \"\"\" + \"```Job title: \" + title + \".\\n\\n \"+ bodytext + \" URL: \" + domain_name + current_path + \" \\n```\"\n",
    "    \n",
    "    return text_prompt\n",
    "\n",
    "#display(Markdown(chatgpt_prompt(title, bodytext)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf021b0",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77a1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting variables for filtering criteria on the website.\n",
    "\n",
    "if job_types != \"\":\n",
    "    url_path = insert_filter(url_path, \"&job_types=\" + job_types)\n",
    "    \n",
    "if categories != \"\":\n",
    "    url_path = insert_filter(url_path, \"&categories=\" + categories)\n",
    "\n",
    "if remote != \"\":    \n",
    "    url_path = define_remote(url_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86a4cb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b940a",
   "metadata": {},
   "source": [
    "## > Scraping url's\n",
    "Mining url's from main site by filtered criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2289b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of websites with job different job descriptions is obtained after filtering.\n",
    "scraped_url_paths = [element['href'] for element in scraping_css_object(url_path, css_object_class)]\n",
    "\n",
    "# Example: Visualization of the complete url\n",
    "#domain_name + scraped_url_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ea26e",
   "metadata": {},
   "source": [
    "## > Scraping information from each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f9f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [21:12<00:00, 12.72s/it]\n"
     ]
    }
   ],
   "source": [
    "json_list = []\n",
    "\n",
    "for current_path in tqdm(scraped_url_paths):\n",
    "    try:\n",
    "        # Scraping job title\n",
    "        title = scrape_title(current_path)\n",
    "\n",
    "        # Scraping job description\n",
    "        bodytext = scrape_job_description(current_path)\n",
    "\n",
    "        # Redacting prompt for ChatGPT\n",
    "        text_prompt = chatgpt_prompt(title, bodytext)\n",
    "\n",
    "        # Calling ChatGPT\n",
    "        reply = call_openai_api(text_prompt, tokens = 1000)\n",
    "        reply = reply.replace(\"\\n\", \"\") \n",
    "\n",
    "        json_object = json.loads(reply)\n",
    "\n",
    "        # Collecting JSON objects\n",
    "        json_list.append(json_object)\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3564493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230609_Data+Scientist_Full+time+role_remote_true\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now()\n",
    "formatted_date = current_date.strftime(\"%y%m%d\")\n",
    "\n",
    "filename = formatted_date + '_' + categories + \"_\" + job_types + \"_\" + \"remote_\" + remote\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc2190b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting JSON file\n",
    "with open(filename + '.json', 'w') as f:\n",
    "    json.dump(json_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130c969",
   "metadata": {},
   "source": [
    "# Transforming JSON file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fdbddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"230609_Data+Scientist_Full+time+role_remote_true.json\"\n",
    "json_file = json.load(open(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7ba240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 15)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=json_file[0].keys())\n",
    "\n",
    "for i in range(len(json_file)):\n",
    "    y = pd.json_normalize(json_file[i])\n",
    "    \n",
    "    # Patch:\n",
    "    y.columns= y.columns.str.lower()\n",
    "\n",
    "    df = pd.concat([df, y], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(\"{}.csv\".format(filename), index=False,  sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546dee6",
   "metadata": {},
   "source": [
    "# Displaying individual JSON objects as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77fdf76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <style>\n",
       "      table {\n",
       "        width: 60%;\n",
       "        border-collapse: collapse;\n",
       "      }\n",
       "      th, td {\n",
       "        padding: 8px;\n",
       "        border-bottom: 1px solid #ddd;\n",
       "        word-wrap: break-word;\n",
       "      }\n",
       "      th:nth-child(2), td:nth-child(2) {\n",
       "        width: 400px;\n",
       "      }\n",
       "    </style>\n",
       "    <table border=\"1\"><tr><th>job_title</th><td>2024 California Regional Office Clerkship</td></tr><tr><th>company_name</th><td>Earthjustice</td></tr><tr><th>company_mission</th><td>To protect the environment and promote environmental justice through lawsuits and administrative advocacy under federal and California environmental laws.</td></tr><tr><th>company_values</th><td>None</td></tr><tr><th>company_products_or_services</th><td>None</td></tr><tr><th>job_responsibilities</th><td>Working alongside attorneys and advocates in San Francisco and Los Angeles offices, participating in the Diversity and Inclusion Fellowship Program organized by the Environmental Law Section of the California Lawyers Association (CLA Section).</td></tr><tr><th>desired_software_skills</th><td>None</td></tr><tr><th>education</th><td>Law degree.</td></tr><tr><th>required_job_experience</th><td>None specified.</td></tr><tr><th>equal_employment_opportunity</th><td>Equal opportunity employer and considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, or any other legally protected status.</td></tr><tr><th>salary</th><td>None</td></tr><tr><th>benefits</th><td>None</td></tr><tr><th>location</th><td>San Francisco and Los Angeles, California.</td></tr><tr><th>type_of_employment</th><td>Summer law clerkship.</td></tr><tr><th>url</th><td>https://climatebase.org/job/46569513/2024-california-regional-office-clerkship?source=jobs_directory</td></tr></table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert JSON to HTML\n",
    "json_object = json_file[-1]\n",
    "html_table = json2html.convert(json.dumps(json_object))\n",
    "\n",
    "display_json_as_html = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <style>\n",
    "      table {{\n",
    "        width: 60%;\n",
    "        border-collapse: collapse;\n",
    "      }}\n",
    "      th, td {{\n",
    "        padding: 8px;\n",
    "        border-bottom: 1px solid #ddd;\n",
    "        word-wrap: break-word;\n",
    "      }}\n",
    "      th:nth-child(2), td:nth-child(2) {{\n",
    "        width: 400px;\n",
    "      }}\n",
    "    </style>\n",
    "    {html_table}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(display_json_as_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
